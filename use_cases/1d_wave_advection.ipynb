{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xno.data.datasets.hdf5_dataset import H5pyDataset\n",
    "from utils import MatReader\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, default_collate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from xno.models import XNO\n",
    "from xno.utils import count_model_params\n",
    "from xno.training import AdamW\n",
    "from xno.training.incremental import IncrementalFNOTrainer\n",
    "from xno.data.transforms.data_processors import IncrementalDataProcessor\n",
    "from xno import LpLoss, H1Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"data/train_IC2.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Model configurations \"\"\"\n",
    "\n",
    "ntrain = 900\n",
    "ntest = 100\n",
    "\n",
    "batch_size = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "epochs = 500\n",
    "step_size = 50   # weight-decay step size\n",
    "gamma = 0.5      # weight-decay rate\n",
    "\n",
    "wavelet = 'db6'  # wavelet basis function\n",
    "level = 3        # lavel of wavelet decomposition\n",
    "width = 96       # uplifting dimension\n",
    "layers = 4       # no of wavelet layers\n",
    "\n",
    "h = 40           # total grid size divided by the subsampling rate\n",
    "grid_range = 1\n",
    "in_channel = 2   # (a(x), x) for this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data is of the shape (number of samples, grid size)\n",
    "data = np.load(data_path)\n",
    "x, t, u_train = data[\"x\"], data[\"t\"], data[\"u\"]  # N x nt x nx\n",
    "\n",
    "x_data = u_train[:, 0, :]  # N x nx, initial solution\n",
    "y_data = u_train[:, -2, :]  # N x nx, final solution\n",
    "\n",
    "x_data = torch.tensor(x_data)\n",
    "y_data = torch.tensor(y_data)\n",
    "\n",
    "x_train = x_data[:ntrain,:]\n",
    "y_train = y_data[:ntrain,:]\n",
    "x_test = x_data[-ntest:,:]\n",
    "y_test = y_data[-ntest:,:]\n",
    "\n",
    "x_train = x_train[:, :, None]\n",
    "x_test = x_test[:, :, None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom Dataset\n",
    "class DictDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'x': self.x[idx], 'y': self.y[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([900, 40, 1]),\n",
       " torch.Size([900, 40]),\n",
       " torch.Size([100, 40, 1]),\n",
       " torch.Size([100, 40]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.permute(0, 2, 1)\n",
    "y_train = y_train.unsqueeze(1)\n",
    "x_test = x_test.permute(0, 2, 1)\n",
    "y_test = y_test.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([900, 1, 40]),\n",
       " torch.Size([900, 1, 40]),\n",
       " torch.Size([100, 1, 40]),\n",
       " torch.Size([100, 1, 40]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DictDataset(x_train, y_train)\n",
    "test_loader = DictDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_loader, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_loader, batch_size=batch_size, shuffle=True)\n",
    "test_loader = {\n",
    "    40: test_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.utils.data.dataloader.DataLoader,\n",
       " dict,\n",
       " torch.Size([20, 1, 40]),\n",
       " torch.Size([20, 1, 40]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "type(train_loader), type(batch), batch['x'].shape, batch['y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict, dict, torch.Size([20, 1, 40]), torch.Size([20, 1, 40]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(test_loader[40]))\n",
    "type(test_loader), type(batch), batch['x'].shape, batch['y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Selected Kernel Description =======\n",
      "Dimentionality: 1D\n",
      "Transformation: [ Laplace Neural Operator (LNO) Kernel ]\n",
      ">>> Overview:\n",
      "The LNO uses a pole-residue formulation to compute solutions to PDEs in the Laplace domain.\n",
      "This kernel is highly effective for problems requiring stability and steady-state solutions.\n",
      "\n",
      ">>> Key Features:\n",
      "- Specially designed for systems dominated by Laplacian dynamics.\n",
      "- Balances transient and steady-state components.\n",
      "\n",
      ">>> Reference:\n",
      "Cao, Q. et al. 'LNO: Laplace Neural Operator for Solving Differential Equations'.\n",
      "Link: https://arxiv.org/pdf/2303.10528\n",
      "============================================\n",
      "\n",
      "================== Config ==================\n",
      ">>> Normaliztion: group_norm\n",
      ">>> Activation Function: \n",
      "============================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = XNO(\n",
    "    max_n_modes=(16, ),\n",
    "    n_modes=(16, ),\n",
    "    hidden_channels=width,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    transformation=\"lno\",\n",
    "    # transformation_kwargs={\"wavelet_level\": level, \"wavelet_size\": [40], \"wavelet_filter\":[wavelet]}, \n",
    "    n_layers=layers\n",
    ")\n",
    "model = model.to(device)\n",
    "n_params = count_model_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=8e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Incre Res: change index to 0\n",
      "Original Incre Res: change sub to 2\n",
      "Original Incre Res: change res to 1024\n"
     ]
    }
   ],
   "source": [
    "data_transform = IncrementalDataProcessor(\n",
    "    in_normalizer=None,\n",
    "    out_normalizer=None,\n",
    "    device=device,\n",
    "    subsampling_rates=[2, 1],\n",
    "    dataset_resolution=2048,\n",
    "    dataset_indices=[2],\n",
    "    epoch_gap=10,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "data_transform = data_transform.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### N PARAMS ###\n",
      " 2473441\n",
      "\n",
      "### OPTIMIZER ###\n",
      " AdamW (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    correct_bias: True\n",
      "    eps: 1e-06\n",
      "    initial_lr: 0.008\n",
      "    lr: 0.008\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "\n",
      "### SCHEDULER ###\n",
      " <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x149d52b30>\n",
      "\n",
      "### LOSSES ###\n",
      "\n",
      "### INCREMENTAL RESOLUTION + GRADIENT EXPLAINED ###\n",
      "\n",
      " * Train: <xno.losses.data_losses.H1Loss object at 0x149d528f0>\n",
      "\n",
      " * Test: {'h1': <xno.losses.data_losses.H1Loss object at 0x149d528f0>, 'l2': <xno.losses.data_losses.LpLoss object at 0x14bc96260>}\n"
     ]
    }
   ],
   "source": [
    "l2loss = LpLoss(d=2, p=2)\n",
    "h1loss = H1Loss(d=2)\n",
    "train_loss = h1loss\n",
    "eval_losses = {\"h1\": h1loss, \"l2\": l2loss}\n",
    "print(\"\\n### N PARAMS ###\\n\", n_params)\n",
    "print(\"\\n### OPTIMIZER ###\\n\", optimizer)\n",
    "print(\"\\n### SCHEDULER ###\\n\", scheduler)\n",
    "print(\"\\n### LOSSES ###\")\n",
    "print(\"\\n### INCREMENTAL RESOLUTION + GRADIENT EXPLAINED ###\")\n",
    "print(f\"\\n * Train: {train_loss}\")\n",
    "print(f\"\\n * Test: {eval_losses}\")\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally pass all of these to the Trainer\n",
    "trainer = IncrementalFNOTrainer(\n",
    "    model=model,\n",
    "    n_epochs=10,\n",
    "    data_processor=data_transform,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    "    incremental_loss_gap=False,\n",
    "    incremental_grad=True,\n",
    "    incremental_grad_eps=0.9999,\n",
    "    incremental_loss_eps = 0.001,\n",
    "    incremental_buffer=5,\n",
    "    incremental_max_iter=1,\n",
    "    incremental_grad_max_iter=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 900 samples\n",
      "Testing on [100] samples         on resolutions [40].\n",
      "Raw outputs of shape torch.Size([20, 1, 20])\n",
      "[0] time=16.54, avg_loss=0.7486, train_err=14.9711\n",
      "Eval: 40_h1=0.9052, 40_l2=0.4813\n",
      "[Rank 0]: saved training state to save/lorenz1d_lno\n",
      "[1] time=16.19, avg_loss=0.5854, train_err=11.7079\n",
      "Eval: 40_h1=0.8430, 40_l2=0.4560\n",
      "[2] time=19.20, avg_loss=0.5757, train_err=11.5131\n",
      "Eval: 40_h1=0.7817, 40_l2=0.4850\n",
      "[3] time=16.47, avg_loss=0.5460, train_err=10.9193\n",
      "Eval: 40_h1=0.7836, 40_l2=0.4706\n",
      "[4] time=16.88, avg_loss=0.4959, train_err=9.9184\n",
      "Eval: 40_h1=0.7021, 40_l2=0.3886\n",
      "[5] time=18.98, avg_loss=0.4734, train_err=9.4670\n",
      "Eval: 40_h1=0.6979, 40_l2=0.3936\n",
      "[Rank 0]: saved training state to save/lorenz1d_lno\n",
      "[6] time=17.84, avg_loss=0.4804, train_err=9.6081\n",
      "Eval: 40_h1=0.7262, 40_l2=0.4276\n",
      "[7] time=17.31, avg_loss=0.4577, train_err=9.1535\n",
      "Eval: 40_h1=0.7477, 40_l2=0.4021\n",
      "[8] time=17.16, avg_loss=0.4810, train_err=9.6193\n",
      "Eval: 40_h1=0.7565, 40_l2=0.4356\n",
      "[9] time=17.18, avg_loss=0.4683, train_err=9.3660\n",
      "Eval: 40_h1=0.7125, 40_l2=0.4029\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_err': 9.366039276123047,\n",
       " 'avg_loss': 0.46830196380615235,\n",
       " 'avg_lasso_loss': None,\n",
       " 'epoch_train_time': 17.177791417001572,\n",
       " '40_h1': tensor(0.7125),\n",
       " '40_l2': tensor(0.4029)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    regularizer=False,\n",
    "    training_loss=train_loss,\n",
    "    eval_losses=eval_losses,\n",
    "    save_every=5, \n",
    "    save_dir=\"save/lorenz1d_lno\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# FNO\u001b[39;00m\n\u001b[1;32m      2\u001b[0m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_err\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m6.474369451734755\u001b[39m,\n\u001b[1;32m      3\u001b[0m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.32371847258673775\u001b[39m,\n\u001b[1;32m      4\u001b[0m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_lasso_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch_train_time\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2.864431916998001\u001b[39m,\n\u001b[0;32m----> 6\u001b[0m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m40_h1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mtensor\u001b[49m(\u001b[38;5;241m0.5186\u001b[39m),\n\u001b[1;32m      7\u001b[0m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m40_l2\u001b[39m\u001b[38;5;124m'\u001b[39m: tensor(\u001b[38;5;241m0.2436\u001b[39m)}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tensor' is not defined"
     ]
    }
   ],
   "source": [
    "# FNO\n",
    "{'train_err': 6.474369451734755,\n",
    " 'avg_loss': 0.32371847258673775,\n",
    " 'avg_lasso_loss': None,\n",
    " 'epoch_train_time': 2.864431916998001,\n",
    " '40_h1': tensor(0.5186),\n",
    " '40_l2': tensor(0.2436)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HNO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WNO\n",
    "{'train_err': 6.527459494272867,\n",
    " 'avg_loss': 0.3263729747136434,\n",
    " 'avg_lasso_loss': None,\n",
    " 'epoch_train_time': 14.213203417006298,\n",
    " '40_h1': tensor(1.2256),\n",
    " '40_l2': tensor(0.8130)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LNO\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
