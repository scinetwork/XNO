{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, default_collate\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from utils import MatReader\n",
    "from pathlib import Path\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xno.models import XNO\n",
    "from xno.data.datasets import Burgers1dTimeDataset\n",
    "from xno.utils import count_model_params\n",
    "from xno.training import AdamW\n",
    "from xno.training.incremental import IncrementalXNOTrainer\n",
    "from xno.data.transforms.data_processors import IncrementalDataProcessor\n",
    "from xno import LpLoss, H1Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom Dataset\n",
    "class DictDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'x': self.x[idx], 'y': self.y[idx]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Burgers 1D dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = 1000\n",
    "ntest = 100\n",
    "sub = 2**3 #subsampling rate\n",
    "h = 2**13 // sub #total grid size divided by the subsampling rate\n",
    "s = h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Trainer Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/burgers_data_R10.mat'\n",
    "batch_size = 20\n",
    "dataset_resolution = 1024\n",
    "\n",
    "# XNO (model) \n",
    "max_modes = (16, )\n",
    "n_modes = (16, )\n",
    "in_channels = 1\n",
    "out_channels = 1\n",
    "n_layers = 4\n",
    "hidden_channels = 64\n",
    "transformation = \"fno\"\n",
    "kwargs = {\n",
    "    \"wavelet_level\": 6, \n",
    "    \"wavelet_size\": [dataset_resolution], \"wavelet_filter\": ['db6']\n",
    "} if transformation.lower() == \"wno\" else {}\n",
    "conv_non_linearity = F.gelu\n",
    "mlp_non_linearity = F.gelu\n",
    "\n",
    "# AdamW (optimizer) \n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-4\n",
    "# CosineAnnealingLR (scheduler) \n",
    "step_size = 50\n",
    "gamma = 0.5\n",
    "\n",
    "# IncrementalDataProcessor (data_transform) \n",
    "dataset_resolution = dataset_resolution\n",
    "dataset_indices = [2]\n",
    "\n",
    "# IncrementalXNOTrainer (trainer) \n",
    "n_epochs = 5 # 500\n",
    "save_every = 5\n",
    "save_testing = True\n",
    "save_dir = f\"save/1d_lorenz/{transformation.lower()}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = MatReader(data_path)\n",
    "x_data = dataloader.read_field('a')[:,::sub]\n",
    "y_data = dataloader.read_field('u')[:,::sub]\n",
    "\n",
    "x_train = x_data[:ntrain,:]\n",
    "y_train = y_data[:ntrain,:]\n",
    "x_test = x_data[-ntest:,:]\n",
    "y_test = y_data[-ntest:,:]\n",
    "\n",
    "x_train = x_train.reshape(ntrain,s,1)\n",
    "x_test = x_test.reshape(ntest,s,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*** Data shape after importing from raw dataset ***\")\n",
    "print(f\"X_Train Shape: {x_train.shape}\")\n",
    "print(f\"Y_Train Shape: {y_train.shape}\")\n",
    "print(f\"X_Test Shape: {x_test.shape}\")\n",
    "print(f\"Y_Test Shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.permute(0, 2, 1)\n",
    "y_train = y_train.unsqueeze(1)\n",
    "x_test = x_test.permute(0, 2, 1)\n",
    "y_test = y_test.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*** Data shape after reshaping based on [Batch, Channel, D1, D2, ...] ***\")\n",
    "print(f\"X_Train Shape: {x_train.shape}\")\n",
    "print(f\"Y_Train Shape: {y_train.shape}\")\n",
    "print(f\"X_Test Shape: {x_test.shape}\")\n",
    "print(f\"Y_Test Shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DictDataset(x_train, y_train)\n",
    "test_loader = DictDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_loader, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_loader, batch_size=batch_size, shuffle=True)\n",
    "test_loader = {\n",
    "    dataset_resolution: test_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*** One batch of the Train Loader ***\")\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"Loader Type: {type(train_loader)}\\nBatch Type: { type(batch)}\\nBatch['x'].shape: {batch['x'].shape}\\nBatch['y'].shape: {batch['y'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*** One batch of the Test Loader ***\")\n",
    "batch = next(iter(test_loader[dataset_resolution]))\n",
    "print(f\"Loader Type: {type(test_loader[dataset_resolution])}\\nBatch Type: { type(batch)}\\nBatch['x'].shape: {batch['x'].shape}\\nBatch['y'].shape: {batch['y'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"*** Device: {device} ***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XNO(\n",
    "    max_n_modes=max_modes,\n",
    "    n_modes=n_modes,\n",
    "    hidden_channels=hidden_channels,\n",
    "    in_channels=in_channels,\n",
    "    out_channels=out_channels,\n",
    "    transformation=transformation,\n",
    "    transformation_kwargs=kwargs,\n",
    "    conv_non_linearity=conv_non_linearity, \n",
    "    mlp_non_linearity=mlp_non_linearity,\n",
    "    n_layers=n_layers\n",
    ")\n",
    "model = model.to(device)\n",
    "n_params = count_model_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(\n",
    "    model.parameters(), \n",
    "    lr=learning_rate, \n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, \n",
    "    step_size=step_size, # default=30\n",
    "    gamma=gamma # default=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = IncrementalDataProcessor(\n",
    "    in_normalizer=None,\n",
    "    out_normalizer=None,\n",
    "    device=device,\n",
    "    subsampling_rates=[2, 1],\n",
    "    dataset_resolution=dataset_resolution,\n",
    "    dataset_indices=dataset_indices,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "data_transform = data_transform.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2loss = LpLoss(d=2, p=2)\n",
    "h1loss = H1Loss(d=2)\n",
    "train_loss = h1loss\n",
    "eval_losses = {\"h1\": h1loss, \"l2\": l2loss}\n",
    "print(\"\\n### N PARAMS ###\\n\", n_params)\n",
    "print(\"\\n### OPTIMIZER ###\\n\", optimizer)\n",
    "print(\"\\n### SCHEDULER ###\\n\", scheduler)\n",
    "print(\"\\n### LOSSES ###\")\n",
    "print(\"\\n### INCREMENTAL RESOLUTION + GRADIENT EXPLAINED ###\")\n",
    "print(f\"\\n * Train: {train_loss}\")\n",
    "print(f\"\\n * Test: {eval_losses}\")\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally pass all of these to the Trainer\n",
    "trainer = IncrementalXNOTrainer(\n",
    "    model=model,\n",
    "    n_epochs=n_epochs,\n",
    "    data_processor=data_transform,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    "    incremental_loss_gap=False,\n",
    "    incremental_grad=True,\n",
    "    incremental_grad_eps=0.9999,\n",
    "    incremental_loss_eps = 0.001,\n",
    "    incremental_buffer=5,\n",
    "    incremental_max_iter=1,\n",
    "    incremental_grad_max_iter=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    regularizer=False,\n",
    "    training_loss=train_loss,\n",
    "    eval_losses=eval_losses,\n",
    "    save_every=save_every,\n",
    "    save_testing=save_testing, \n",
    "    save_dir=save_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FNO\n",
    "{'train_err': 0.9652893996238708,\n",
    " 'avg_loss': 0.04826446998119354,\n",
    " 'avg_lasso_loss': None,\n",
    " 'epoch_train_time': 27.041632332999143,\n",
    " '1024_h1': tensor(0.0500),\n",
    " '1024_l2': tensor(0.0418)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HNO\n",
    "{'train_err': 1.0265669953823089,\n",
    " 'avg_loss': 0.05132834976911545,\n",
    " 'avg_lasso_loss': None,\n",
    " 'epoch_train_time': 34.57337279099738,\n",
    " '1024_h1': tensor(0.0500),\n",
    " '1024_l2': tensor(0.0445)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LNO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WNO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
