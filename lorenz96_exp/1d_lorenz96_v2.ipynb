{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4955ed0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, default_collate\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from utils import MatReader\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69f4f859",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from xno.models import XNO\n",
    "from xno.data.datasets import Burgers1dTimeDataset\n",
    "from xno.utils import count_model_params\n",
    "from xno.training import AdamW\n",
    "from xno.training.incremental import IncrementalXNOTrainer\n",
    "from xno.data.transforms.data_processors import IncrementalDataProcessor\n",
    "from xno import LpLoss, H1Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0472095d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define the custom Dataset\n",
    "class DictDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'x': self.x[idx], 'y': self.y[idx]}\n",
    "\n",
    "# # Loading Burgers 1D dataset\n",
    "\n",
    "# ## Settings\n",
    "\n",
    "# ### Data Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46d2c361",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = 400\n",
    "ntest = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1ee0abc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "data_path = 'data/lorenz96_dataset_v2.mat'\n",
    "data_name = '1d_lorenz96'\n",
    "time_step_mode = \"s\"\n",
    "\n",
    "batch_size = 20\n",
    "channels = 40\n",
    "dataset_resolution = 150\n",
    "\n",
    "# XNO (model) \n",
    "max_modes = (16, )\n",
    "n_modes = (16, )\n",
    "in_channels = 40\n",
    "out_channels = 40\n",
    "n_layers = 4\n",
    "hidden_channels = 32\n",
    "transformation = \"fno\"\n",
    "kwargs = {\n",
    "    \"wavelet_level\": 3, \n",
    "    \"wavelet_size\": [dataset_resolution], \"wavelet_filter\": ['db6']\n",
    "} if transformation.lower() == \"wno\" else {}\n",
    "\n",
    "conv_non_linearity = None\n",
    "mlp_non_linearity = None\n",
    "\n",
    "match transformation.lower():\n",
    "    case \"fno\" | \"hno\":\n",
    "        conv_non_linearity = F.gelu\n",
    "        mlp_non_linearity = F.gelu\n",
    "    case \"wno\":\n",
    "        conv_non_linearity = F.gelu\n",
    "        mlp_non_linearity = F.gelu\n",
    "    case \"lno\":\n",
    "        conv_non_linearity = torch.sin\n",
    "        mlp_non_linearity = torch.tanh\n",
    "\n",
    "# AdamW (optimizer) \n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-4\n",
    "# CosineAnnealingLR (scheduler) \n",
    "step_size = 100 if transformation.lower() == \"lno\" else 50\n",
    "gamma = 0.5\n",
    "\n",
    "# IncrementalDataProcessor (data_transform) \n",
    "dataset_resolution = dataset_resolution\n",
    "dataset_indices = [2]\n",
    "\n",
    "# IncrementalXNOTrainer (trainer) \n",
    "n_epochs = 250 # 500\n",
    "save_every = 50\n",
    "save_testing = True\n",
    "save_dir = f\"save/{data_name}/{transformation.lower()}/\"\n",
    "\n",
    "\n",
    "# Open the file at the start of the script\n",
    "# output_file = open(f\"{data_name}_{transformation.lower()}.txt\", \"w\")\n",
    "# sys.stdout = output_file  # Redirect stdout to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa78b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load dataset\n",
    "reader = MatReader(data_path)\n",
    "\n",
    "X = reader.read_field(\"data\")  # shape [T, D], e.g. [2001, 40]\n",
    "X = X.permute(2, 1, 0)\n",
    "\n",
    "sub = 1\n",
    "T_in = 150\n",
    "T = 150\n",
    "\n",
    "# 2) Shape X & Y, Test & Train\n",
    "x_ns = X[::sub, ::sub, :T_in]    # First T_in steps\n",
    "y_ns = X[::sub, ::sub, T_in:T_in+T]      # Next T_in + T steps \n",
    "\n",
    "x_train, x_test = x_ns[:ntrain], x_ns[ntrain:ntrain+ntest]\n",
    "y_train, y_test = y_ns[:ntrain], y_ns[ntrain:ntrain+ntest]\n",
    "\n",
    "# 3) Print shapes to verify\n",
    "print(\"Sshapes:\", x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bc1654",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"\\n=== Data shape after importing from raw dataset ===\\n\")\n",
    "print(f\"X_Train Shape: {x_train.shape}\")\n",
    "print(f\"Y_Train Shape: {y_train.shape}\")\n",
    "print(f\"X_Test Shape: {x_test.shape}\")\n",
    "print(f\"Y_Test Shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec9cd76b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# x_train = x_train.permute(0, 2, 1)\n",
    "# x_train = x_train.unsqueeze(1)\n",
    "# y_train = y_train.unsqueeze(1)\n",
    "# x_test = x_test.permute(0, 2, 1)\n",
    "# x_test = x_test.unsqueeze(1)\n",
    "# y_test = y_test.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e99142",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"\\n=== Data shape after reshaping based on [Batch, Channel, D1, D2, ...] ===\\n\")\n",
    "print(f\"X_Train Shape: {x_train.shape}\")\n",
    "print(f\"Y_Train Shape: {y_train.shape}\")\n",
    "print(f\"X_Test Shape: {x_test.shape}\")\n",
    "print(f\"Y_Test Shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "083c7e45",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "train_loader = DictDataset(x_train, y_train)\n",
    "test_loader = DictDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d9f3553",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_loader, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_loader, batch_size=batch_size, shuffle=True)\n",
    "test_loader = {\n",
    "    dataset_resolution: test_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ec87fc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"\\n=== One batch of the Train Loader ===\\n\")\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"Loader Type: {type(train_loader)}\\nBatch Type: { type(batch)}\\nBatch['x'].shape: {batch['x'].shape}\\nBatch['y'].shape: {batch['y'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce01e150",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"\\n=== One batch of the Test Loader ===\\n\")\n",
    "batch = next(iter(test_loader[dataset_resolution]))\n",
    "print(f\"Loader Type: {type(test_loader[dataset_resolution])}\\nBatch Type: { type(batch)}\\nBatch['x'].shape: {batch['x'].shape}\\nBatch['y'].shape: {batch['y'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f08701",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\n=== Device: {device} ===\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415523a7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "model = XNO(\n",
    "    max_n_modes=max_modes,\n",
    "    n_modes=n_modes,\n",
    "    hidden_channels=hidden_channels,\n",
    "    in_channels=in_channels,\n",
    "    out_channels=out_channels,\n",
    "    transformation=transformation,\n",
    "    transformation_kwargs=kwargs,\n",
    "    conv_non_linearity=conv_non_linearity, \n",
    "    mlp_non_linearity=mlp_non_linearity,\n",
    "    n_layers=n_layers\n",
    ")\n",
    "model = model.to(device)\n",
    "n_params = count_model_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ad42b86",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(\n",
    "    model.parameters(), \n",
    "    lr=learning_rate, \n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, \n",
    "    step_size=step_size, # default=30\n",
    "    gamma=gamma # default=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb6924d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "data_transform = IncrementalDataProcessor(\n",
    "    in_normalizer=None,\n",
    "    out_normalizer=None,\n",
    "    device=device,\n",
    "    subsampling_rates=[2, 1],\n",
    "    dataset_resolution=dataset_resolution,\n",
    "    dataset_indices=dataset_indices,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "data_transform = data_transform.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ea9ae1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "l2loss = LpLoss(d=2, p=2,)\n",
    "h1loss = H1Loss(d=2)\n",
    "train_loss = h1loss\n",
    "eval_losses = {\"h1\": h1loss, \"l2\": l2loss}\n",
    "print(\"\\n### N PARAMS ###\\n\", n_params)\n",
    "print(\"\\n### OPTIMIZER ###\\n\", optimizer)\n",
    "print(\"\\n### SCHEDULER ###\\n\", scheduler)\n",
    "print(\"\\n### LOSSES ###\")\n",
    "print(\"\\n### INCREMENTAL RESOLUTION + GRADIENT EXPLAINED ###\")\n",
    "print(f\"\\n * Train: {train_loss}\")\n",
    "print(f\"\\n * Test: {eval_losses}\")\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "352b2b22",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Finally pass all of these to the Trainer\n",
    "trainer = IncrementalXNOTrainer(\n",
    "    model=model,\n",
    "    n_epochs=n_epochs,\n",
    "    data_processor=data_transform,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    "    incremental_loss_gap=False,\n",
    "    incremental_grad=True,\n",
    "    incremental_grad_eps=0.9999,\n",
    "    incremental_loss_eps = 0.001,\n",
    "    incremental_buffer=5,\n",
    "    incremental_max_iter=1,\n",
    "    incremental_grad_max_iter=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70091b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mess = trainer.train(\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    regularizer=False,\n",
    "    training_loss=train_loss,\n",
    "    eval_losses=eval_losses,\n",
    "    # save_every=save_every,\n",
    "    # save_testing=save_testing, \n",
    "    # save_dir=save_dir\n",
    ")\n",
    "\n",
    "print(mess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5397fb73",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "# At the end of the script\n",
    "sys.stdout = sys.__stdout__  # Restore original stdout\n",
    "output_file.close()  # Close the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e76366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
